{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "u5eq2o0aP6Zm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5eq2o0aP6Zm",
        "outputId": "3faeae11-b138-43b5-efe0-9021a41da6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0bcb7cd06caa928388b905c6edcd853ba24542a55b6701363544c751a950ad57\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.14\n"
          ]
        }
      ],
      "source": [
        "!pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1J_Q5ybSSop_",
      "metadata": {
        "id": "1J_Q5ybSSop_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "325a1c1f",
      "metadata": {
        "id": "325a1c1f"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import Process, Queue\n",
        "from num2words import num2words\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import multiprocessing\n",
        "import nltk\n",
        "import os\n",
        "import os, random, string, re\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import torch.multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7a49b71f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a49b71f",
        "outputId": "2bc41509-3689-47c7-c7f1-d4f2d30f14dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'tus-santos' created successfully at /content/drive/MyDrive/NLP/original-tables/tus-santos\n",
            "Folder 'wiki-jaccard' created successfully at /content/drive/MyDrive/NLP/original-tables/wiki-jaccard\n",
            "Folder 'wiki-containment' created successfully at /content/drive/MyDrive/NLP/original-tables/wiki-containment\n"
          ]
        }
      ],
      "source": [
        "# copy datsets tar.gz files from https://zenodo.org/records/10042019 under /content/drive/MyDrive/NLP/original-tables/\n",
        "# prompt: create folders under /content/drive/MyDrive/NLP/original-tables/ called tus-santos, wiki-jaccard, wiki-containment\n",
        "\n",
        "\n",
        "folders_to_create = [\"tus-santos\", \"wiki-jaccard\", \"wiki-containment\"]\n",
        "base_path = \"/content/drive/MyDrive/NLP/original-tables/\"\n",
        "\n",
        "for folder in folders_to_create:\n",
        "  path = os.path.join(base_path, folder)\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print(f\"Folder '{folder}' created successfully at {path}\")\n",
        "  else:\n",
        "    print(f\"Folder '{folder}' already exists at {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b103c0a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b103c0a1",
        "outputId": "b7d8b097-d3fa-41ba-c002-00d896049d48"
      },
      "outputs": [],
      "source": [
        "# prompt: unzip tar file\n",
        "!tar -xvf /content/drive/MyDrive/NLP/original-tables/tus-santos.tar.bz2 -C /content/drive/MyDrive/NLP/original-tables/tus-santos\n",
        "!tar -xvf /content/drive/MyDrive/NLP/original-tables/wiki-jaccard.tar.bz2 -C /content/drive/MyDrive/NLP/original-tables/wiki-jaccard\n",
        "!tar -xvf /content/drive/MyDrive/NLP/original-tables/wiki-containment.tar.bz2 -C /content/drive/MyDrive/NLP/original-tables/wiki-containment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "395bdf40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "395bdf40",
        "outputId": "a46ffd3e-527b-4c86-f3c5-a8870e4727b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "cpu_cores = [ i for i in range(70)]\n",
        "\n",
        "os.sched_setaffinity(os.getpid(), cpu_cores)\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "def analyze_column_values(df, column_name):\n",
        "    value_counts = df[column_name].astype(str).value_counts()\n",
        "\n",
        "    sorted_values = value_counts.index.tolist()\n",
        "\n",
        "    n = len(sorted_values)\n",
        "    col = ', '.join(sorted_values)\n",
        "\n",
        "    lengths = [len(str(value)) for value in sorted_values]\n",
        "    max_len = max(lengths)\n",
        "    min_len = min(lengths)\n",
        "    avg_len = sum(lengths) / len(lengths)\n",
        "    tokens = f\"{column_name} contains {str(n)} values ({str(max_len)}, {str(min_len)}, {str(avg_len)}): {col}\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(tokens)\n",
        "    truncated_tokens = tokens[:512]\n",
        "    truncated_sentence = ' '.join(truncated_tokens)\n",
        "    return truncated_sentence\n",
        "\n",
        "def evaluate4(df):\n",
        "    columns = df.columns.tolist()\n",
        "    sentens_list = []\n",
        "    for column in columns:\n",
        "        s = analyze_column_values(df,column)\n",
        "        sentens_list.append(s)\n",
        "    return sentens_list\n",
        "\n",
        "def get_file_columns(file_path):\n",
        "    df = pd.read_csv(file_path,engine='python',nrows =1)\n",
        "    columns = len(df.columns)\n",
        "    return columns\n",
        "\n",
        "def partition_files(file_paths, m):\n",
        "    random.shuffle(file_paths)\n",
        "    file_info = []\n",
        "\n",
        "    current_group = []\n",
        "    current_columns = 0\n",
        "    stime = time.time()\n",
        "    for file_path in file_paths:\n",
        "        columns = get_file_columns(file_path)\n",
        "        if current_columns + columns > m:\n",
        "            file_info.append(current_group)\n",
        "            current_group = [file_path + \"_\" + str(columns)]\n",
        "            current_columns = columns\n",
        "        else:\n",
        "            current_group.append(file_path+ \"_\" + str(columns))\n",
        "            current_columns += columns\n",
        "    endtime = time.time()\n",
        "    if current_group:\n",
        "        file_info.append(current_group)\n",
        "    return file_info\n",
        "\n",
        "def create_folder(path):\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "        print(\"Folder and its content deleted.\")\n",
        "\n",
        "    os.makedirs(path)\n",
        "    print(\"Folder created.\")\n",
        "\n",
        "def read_pkl_files(folder_path):\n",
        "    file_list = os.listdir(folder_path)\n",
        "\n",
        "    re_dict = {}\n",
        "\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if file_name.endswith(\".pkl\"):\n",
        "            try:\n",
        "                with open(file_path, 'rb') as file:\n",
        "                    obj = pickle.load(file)\n",
        "\n",
        "                re_dict.update(obj)\n",
        "            except Exception as e:\n",
        "                print(\"Error occurred while reading\", file_name, \":\", str(e))\n",
        "    return re_dict\n",
        "\n",
        "def process_task4(i,input_values,queue,queue_inforgather,file_dic_path):\n",
        "\n",
        "    dict = {}\n",
        "    for input_value in input_values:\n",
        "        k = struct_dic_key(input_value)\n",
        "        try:\n",
        "            df = pd.read_csv(input_value, low_memory=False)\n",
        "        except Exception as e:\n",
        "            print(\"error filename:\",input_value)\n",
        "            continue\n",
        "        embdings = evaluate4(df)\n",
        "        dict[k] = embdings\n",
        "        queue.put(1)\n",
        "    filename = os.path.join(file_dic_path,str(i)+\".pkl\")\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(dict, file)\n",
        "    queue.put((-1, \"test-pid\"))\n",
        "\n",
        "def struct_dic_key(filepath):\n",
        "    elelist = filepath.split(os.sep)\n",
        "    return elelist[-2] + \"-\" + elelist[-1]\n",
        "\n",
        "def split_list(lst, num_parts):\n",
        "    avg = len(lst) // num_parts\n",
        "    remainder = len(lst) % num_parts\n",
        "\n",
        "    result = []\n",
        "    start = 0\n",
        "    for i in range(num_parts):\n",
        "        if i < remainder:\n",
        "            end = start + avg + 1\n",
        "        else:\n",
        "            end = start + avg\n",
        "        result.append(lst[start:end])\n",
        "        start = end\n",
        "\n",
        "    return result\n",
        "\n",
        "def process_table_sentense(filepathstore,datadir,data_pkl_name,tmppath= \"/data/lijiajun/webtable_tmp\",split_num=10):\n",
        "    list_of_tuples_name = data_pkl_name\n",
        "    dir = datadir\n",
        "    file_dic_path = tmppath\n",
        "\n",
        "    os.makedirs(filepathstore,exist_ok=True)\n",
        "    create_folder(file_dic_path)\n",
        "\n",
        "    filelist = []\n",
        "\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            if file == 'small_join.csv' or file  == 'large_join.csv':\n",
        "                continue\n",
        "            filepath = os.path.join(root, file)\n",
        "            if os.path.isfile(filepath):\n",
        "                filelist.append(filepath)\n",
        "\n",
        "            else:\n",
        "                print(f\"file: {filepath} is not a file ,pass\")\n",
        "    print(f\"split1 all file ,filelistlen: {len(filelist)} added to filelist\")\n",
        "\n",
        "\n",
        "    inputs = filelist\n",
        "    sub_file_ls = split_list(inputs, split_num)\n",
        "    process_list = []\n",
        "\n",
        "    queues = [Queue() for i in range(split_num)]\n",
        "    finished = [False for i in range(split_num)]\n",
        "\n",
        "    bars = [tqdm(total=len(sub_file_ls[i]), desc=f\"bar-{i}\", position=i) for i in range(split_num)]\n",
        "    results = [None for i in range(split_num)]\n",
        "    queue_inforgather = multiprocessing.Manager().Queue()\n",
        "\n",
        "    for i in range(split_num):\n",
        "        process = Process(target=process_task4, args=(i,sub_file_ls[i], queues[i], queue_inforgather,file_dic_path))\n",
        "        process_list.append(process)\n",
        "        process.start()\n",
        "\n",
        "    while True:\n",
        "        for i in range(split_num):\n",
        "            queue = queues[i]\n",
        "            bar = bars[i]\n",
        "            try:\n",
        "                res = queue.get_nowait()\n",
        "                if isinstance(res, tuple) and res[0] == -1:\n",
        "                    finished[i] = True\n",
        "                    results[i] = res[1]\n",
        "                    continue\n",
        "                bar.update(res)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        if all(finished):\n",
        "            break\n",
        "\n",
        "    for process in process_list:\n",
        "        process.join()\n",
        "\n",
        "    result_dict = read_pkl_files(file_dic_path)\n",
        "\n",
        "    list_of_tuples = list(result_dict.items())\n",
        "\n",
        "    with open(os.path.join(filepathstore,list_of_tuples_name),'wb') as file:\n",
        "        pickle.dump(list_of_tuples,file)\n",
        "    print(\"pickle sucesss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "221923a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "221923a0",
        "outputId": "57f032f2-aed1-47db-e9dd-3d7e02caa545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will process 7884 distinct tables, 12389 pairs total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding tables: 100%|██████████| 7884/7884 [03:59<00:00, 32.88it/s]\n",
            "Scoring pairs: 100%|██████████| 12389/12389 [00:04<00:00, 2614.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Wrote 99917 rows to /content/join_scores_wiki_jaccard.csv\n",
            "[+] Moved result to /content/drive/MyDrive/NLP/join-scores/wiki-jaccard.csv\n"
          ]
        }
      ],
      "source": [
        "# ── CONFIGURATION ────────────────────────────────────────────────────────────\n",
        "DATA_DIR    = \"/content/drive/MyDrive/NLP/original-tables/wiki-jaccard/tables\"\n",
        "LABELS_JSON = \"/content/drive/MyDrive/NLP/original-tables/wiki-jaccard/labels.json\"\n",
        "MODEL_NAME  = \"all-mpnet-base-v2\"\n",
        "LOCAL_OUT   = \"/content/join_scores_wiki_jaccard.csv\"   # write here first\n",
        "DRIVE_OUT   = \"/content/drive/MyDrive/NLP/join-scores/wiki-jaccard.csv\"\n",
        "BATCH_SIZE  = 128  # increase if GPU memory allows\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_pairs(json_path):\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)[\"train\"]\n",
        "    # extract unique (table1, table2) filename pairs\n",
        "    return list({(r[\"table1\"][\"filename\"], r[\"table2\"][\"filename\"]) for r in data})\n",
        "\n",
        "def embed_table(path, model, device):\n",
        "    \"\"\"Read a CSV once, build all column-texts, batch-encode, and return (cols, embs).\"\"\"\n",
        "    df = pd.read_csv(path, dtype=str)  # single read\n",
        "    cols = df.columns.tolist()\n",
        "    texts = [analyze_column_values(df, c) for c in cols]\n",
        "    del df  # free memory\n",
        "    embs = model.encode(\n",
        "        texts,\n",
        "        convert_to_tensor=True,\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "    return cols, embs\n",
        "\n",
        "def main():\n",
        "    # 1) Load table-pairs from labels.json\n",
        "    pairs = load_pairs(LABELS_JSON)\n",
        "    tables = sorted({t for pair in pairs for t in pair})\n",
        "    print(f\"Will process {len(tables)} distinct tables, {len(pairs)} pairs total.\")\n",
        "\n",
        "    # 2) Prepare model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model  = SentenceTransformer(MODEL_NAME, device=device).half()\n",
        "\n",
        "    # 3) Embed each table exactly once\n",
        "    table_cols = {}\n",
        "    table_embs = {}\n",
        "    for t in tqdm(tables, desc=\"Embedding tables\"):\n",
        "        path = os.path.join(DATA_DIR, t)\n",
        "        cols, embs = embed_table(path, model, device)\n",
        "        table_cols[t] = cols\n",
        "        table_embs[t] = embs\n",
        "\n",
        "    # 4) Compute all column-pair scores\n",
        "    rows = []\n",
        "    for t1, t2 in tqdm(pairs, desc=\"Scoring pairs\"):\n",
        "        cols1, e1 = table_cols[t1], table_embs[t1]  # [m, dim]\n",
        "        cols2, e2 = table_cols[t2], table_embs[t2]  # [n, dim]\n",
        "\n",
        "        sims = util.pytorch_cos_sim(e1, e2)  # [m,n]\n",
        "        m, n = sims.shape\n",
        "        flat = sims.reshape(-1)\n",
        "\n",
        "        for idx, score in enumerate(flat):\n",
        "            i, j = divmod(idx, n)\n",
        "            rows.append((t1, cols1[i], t2, cols2[j], score.item()))\n",
        "\n",
        "        # free GPU memory for this matrix\n",
        "        del sims\n",
        "\n",
        "    # 5) Write locally\n",
        "    df_out = pd.DataFrame(rows, columns=[\n",
        "        \"table1\",\"col1\",\"table2\",\"col2\",\"join_score\"\n",
        "    ])\n",
        "    df_out.to_csv(LOCAL_OUT, index=False)\n",
        "    print(f\"[+] Wrote {len(rows)} rows to {LOCAL_OUT}\")\n",
        "\n",
        "    # 6) Move to Drive\n",
        "    shutil.move(LOCAL_OUT, DRIVE_OUT)\n",
        "    print(f\"[+] Moved result to {DRIVE_OUT}\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "256f5f99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "256f5f99",
        "outputId": "2ab97be1-285d-43b6-df17-2c9a3f615809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8489 CSVs; perturbing with 4 workers…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overall: 100%|██████████| 8489/8489 [05:00<00:00, 28.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All done!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# ── CONFIGURATION ───────────────────────────────────────────────────────────\n",
        "DATA_DIR   = \"/content/drive/MyDrive/NLP/original-tables/wiki-jaccard/tables\"\n",
        "PERT_DIR   = \"/content/drive/MyDrive/NLP/perturbed-tables/wiki-jaccard\"\n",
        "CPU_CORES  = 4                  # adjust to your VM’s core count\n",
        "# perturbation rates…\n",
        "TYPO_RATE  = 0.1\n",
        "NULL_RATE  = 0.05\n",
        "DUP_RATE   = 0.1\n",
        "ABRV_RATE  = 0.5\n",
        "SPLIT_RATE = 0.3\n",
        "SYNONYMS   = {\"first_name\":\"fname\",\"last_name\":\"lname\",\"date_of_birth\":\"dob\"}\n",
        "random.seed(42)\n",
        "os.makedirs(PERT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Value-level noise funcs ----\n",
        "def random_insertion(s):\n",
        "    i = random.randrange(len(s)+1); return s[:i]+random.choice(string.ascii_letters)+s[i:]\n",
        "def random_deletion(s):\n",
        "    if len(s)<=1: return s\n",
        "    i = random.randrange(len(s)); return s[:i]+s[i+1:]\n",
        "def random_swap(s):\n",
        "    if len(s)<2: return s\n",
        "    i=random.randrange(len(s)-1); L=list(s); L[i],L[i+1]=L[i+1],L[i]; return \"\".join(L)\n",
        "def random_replacement(s):\n",
        "    if not s: return s\n",
        "    i=random.randrange(len(s)); return s[:i]+random.choice(string.ascii_letters)+s[i+1:]\n",
        "TYPO_FUNCS = [random_insertion, random_deletion, random_swap, random_replacement]\n",
        "\n",
        "def perturb_cell(val):\n",
        "    if pd.isna(val): return val\n",
        "    if isinstance(val,str) and random.random()<TYPO_RATE:\n",
        "        val = random.choice(TYPO_FUNCS)(val)\n",
        "    if random.random()<NULL_RATE:\n",
        "        return \"\"\n",
        "    return val\n",
        "\n",
        "# ---- Schema funcs ----\n",
        "def abbreviate(name):\n",
        "    parts = re.split(r'[^A-Za-z0-9]+', name)\n",
        "    return \"\".join(p[0] for p in parts if p).lower()\n",
        "\n",
        "def rename_headers(headers):\n",
        "    out = {}\n",
        "    for h in headers:\n",
        "        if h in SYNONYMS and random.random()<ABRV_RATE:\n",
        "            out[h]=SYNONYMS[h]\n",
        "        elif random.random()<ABRV_RATE:\n",
        "            out[h]=abbreviate(h)\n",
        "        else:\n",
        "            out[h]=h\n",
        "    return out\n",
        "\n",
        "def split_composite_columns(df):\n",
        "    candidates = [c for c in df.columns if \" \" in c]\n",
        "    k = int(len(candidates)*SPLIT_RATE)\n",
        "    for c in random.sample(candidates, k):\n",
        "        parts = df[c].astype(str).str.split(\" \",1,expand=True)\n",
        "        df[f\"{c}_1\"]=parts[0]; df[f\"{c}_2\"]=parts[1].fillna(\"\")\n",
        "        df.drop(columns=[c],inplace=True)\n",
        "    return df\n",
        "\n",
        "def reorder_columns(df):\n",
        "    cols = df.columns.tolist(); random.shuffle(cols); return df[cols]\n",
        "\n",
        "def duplicate_rows(df):\n",
        "    n = int(len(df)*DUP_RATE)\n",
        "    return pd.concat([df, df.sample(n, replace=False)], ignore_index=True) if n>0 else df\n",
        "\n",
        "def convert_numbers_to_words_df(df, rate=0.8, seed=42):\n",
        "    df = df.copy(); rng = pd.api.types\n",
        "    int_cols = [c for c in df.columns if (\n",
        "        rng.is_integer_dtype(df[c]) or\n",
        "        (rng.is_float_dtype(df[c]) and df[c].dropna().apply(float.is_integer).all())\n",
        "    )]\n",
        "    for col in int_cols:\n",
        "        idx = df[col].dropna().sample(frac=rate, random_state=seed).index\n",
        "        for i in idx:\n",
        "            try: df.at[i,col]=num2words(int(df.at[i,col]))\n",
        "            except: pass\n",
        "        df[col]=df[col].astype(object)\n",
        "    return df\n",
        "\n",
        "# ---- Single‐file worker ----\n",
        "def process_csv(fname):\n",
        "    inp  = os.path.join(DATA_DIR, fname)\n",
        "    outp = os.path.join(PERT_DIR, fname)\n",
        "    df = pd.read_csv(inp, dtype=str).fillna(\"\")\n",
        "\n",
        "    # 1. Value noise (column‐wise map is slightly faster than applymap)\n",
        "    for c in df.columns:\n",
        "        df[c] = df[c].map(perturb_cell)\n",
        "\n",
        "    # 2. Duplicate, rename, reorder, split\n",
        "    df = duplicate_rows(df)\n",
        "    df.rename(columns=rename_headers(df.columns), inplace=True)\n",
        "    df = reorder_columns(df)\n",
        "    df = split_composite_columns(df)\n",
        "\n",
        "    # 3. Numeric→words\n",
        "    df = convert_numbers_to_words_df(df)\n",
        "\n",
        "    # 4. Save\n",
        "    df.to_csv(outp, index=False)\n",
        "    return fname\n",
        "\n",
        "# ---- Main: parallelize over CSVs ----\n",
        "if __name__==\"__main__\":\n",
        "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
        "    print(f\"Found {len(files)} CSVs; perturbing with {CPU_CORES} workers…\")\n",
        "    with ProcessPoolExecutor(max_workers=CPU_CORES) as exe:\n",
        "        for _ in tqdm(exe.map(process_csv, files), total=len(files), desc=\"Overall\"):\n",
        "            pass\n",
        "    print(\"All done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e6ec124e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ec124e",
        "outputId": "373675a3-c8c2-460e-d8dc-c9f7c75f707d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will process 7884 distinct tables, 12389 pairs total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding tables: 100%|██████████| 7884/7884 [04:38<00:00, 28.29it/s]\n",
            "Scoring pairs: 100%|██████████| 12389/12389 [00:05<00:00, 2080.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Wrote 260675 rows to /content/join_scores_wiki_jaccard.csv\n",
            "[+] Moved result to /content/drive/MyDrive/NLP/join-scores/wiki-jaccard-perturbed.csv\n"
          ]
        }
      ],
      "source": [
        "# ── CONFIGURATION ────────────────────────────────────────────────────────────\n",
        "DATA_DIR    = \"/content/drive/MyDrive/NLP/perturbed-tables/wiki-jaccard\"\n",
        "LABELS_JSON = \"/content/drive/MyDrive/NLP/original-tables/wiki-jaccard/labels.json\"\n",
        "MODEL_NAME  = \"all-mpnet-base-v2\"\n",
        "LOCAL_OUT   = \"/content/join_scores_wiki_jaccard.csv\"   # write here first\n",
        "DRIVE_OUT   = \"/content/drive/MyDrive/NLP/join-scores/wiki-jaccard-perturbed.csv\"\n",
        "BATCH_SIZE  = 128  # increase if GPU memory allows\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_pairs(json_path):\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)[\"train\"]\n",
        "    # extract unique (table1, table2) filename pairs\n",
        "    return list({(r[\"table1\"][\"filename\"], r[\"table2\"][\"filename\"]) for r in data})\n",
        "\n",
        "def embed_table(path, model, device):\n",
        "    \"\"\"Read a CSV once, build all column-texts, batch-encode, and return (cols, embs).\"\"\"\n",
        "    df = pd.read_csv(path, dtype=str)  # single read\n",
        "    cols = df.columns.tolist()\n",
        "    texts = [analyze_column_values(df, c) for c in cols]\n",
        "    del df  # free memory\n",
        "    embs = model.encode(\n",
        "        texts,\n",
        "        convert_to_tensor=True,\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "    return cols, embs\n",
        "\n",
        "def main():\n",
        "    # 1) Load table-pairs from labels.json\n",
        "    pairs = load_pairs(LABELS_JSON)\n",
        "    tables = sorted({t for pair in pairs for t in pair})\n",
        "    print(f\"Will process {len(tables)} distinct tables, {len(pairs)} pairs total.\")\n",
        "\n",
        "    # 2) Prepare model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model  = SentenceTransformer(MODEL_NAME, device=device).half()\n",
        "\n",
        "    # 3) Embed each table exactly once\n",
        "    table_cols = {}\n",
        "    table_embs = {}\n",
        "    for t in tqdm(tables, desc=\"Embedding tables\"):\n",
        "        path = os.path.join(DATA_DIR, t)\n",
        "        cols, embs = embed_table(path, model, device)\n",
        "        table_cols[t] = cols\n",
        "        table_embs[t] = embs\n",
        "\n",
        "    # 4) Compute all column-pair scores\n",
        "    rows = []\n",
        "    for t1, t2 in tqdm(pairs, desc=\"Scoring pairs\"):\n",
        "        cols1, e1 = table_cols[t1], table_embs[t1]  # [m, dim]\n",
        "        cols2, e2 = table_cols[t2], table_embs[t2]  # [n, dim]\n",
        "\n",
        "        sims = util.pytorch_cos_sim(e1, e2)  # [m,n]\n",
        "        m, n = sims.shape\n",
        "        flat = sims.reshape(-1)\n",
        "\n",
        "        for idx, score in enumerate(flat):\n",
        "            i, j = divmod(idx, n)\n",
        "            rows.append((t1, cols1[i], t2, cols2[j], score.item()))\n",
        "\n",
        "        # free GPU memory for this matrix\n",
        "        del sims\n",
        "\n",
        "    # 5) Write locally\n",
        "    df_out = pd.DataFrame(rows, columns=[\n",
        "        \"table1\",\"col1\",\"table2\",\"col2\",\"join_score\"\n",
        "    ])\n",
        "    df_out.to_csv(LOCAL_OUT, index=False)\n",
        "    print(f\"[+] Wrote {len(rows)} rows to {LOCAL_OUT}\")\n",
        "\n",
        "    # 6) Move to Drive\n",
        "    shutil.move(LOCAL_OUT, DRIVE_OUT)\n",
        "    print(f\"[+] Moved result to {DRIVE_OUT}\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fc40da1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc40da1d",
        "outputId": "ac32ae19-0377-4692-c6ad-a73a134031c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mean_delta': np.float64(0.026456613731250783), 'mean_abs_delta': np.float64(0.041008747257167695), 'max_abs_delta': 0.3973388671875, 'num_changed': np.int64(25430), 'total_pairs': 25718}\n",
            "                 table1  col1            table2  col2  join_score_gold  \\\n",
            "3351   HD9OLJS8ATA3.csv  col0  RS4XD8J9EP5J.csv  col0         0.161255   \n",
            "2398   U8O6P8YN2K8K.csv  col3  0372NLZGK68Q.csv  col1         0.337891   \n",
            "5782   7DA4PCURSY6B.csv  col1  CP01QJYBZLXM.csv  col0         0.183105   \n",
            "8442   ZRK0YN7LA6RE.csv  col0  NBOTSA7FFMZH.csv  col2         0.433594   \n",
            "21496  KLY8QYGL5Y84.csv  col1  7Q2UOITFNN1U.csv  col1         0.459473   \n",
            "23548  DQBSUQ1E8JAG.csv  col0  KIG7OTFNAHPV.csv  col2         0.313232   \n",
            "3199   OPUHNEXANP6T.csv  col0  NBOTSA7FFMZH.csv  col2         0.425049   \n",
            "17950  U8O6P8YN2K8K.csv  col3  LS1FD31NDF2W.csv  col1         0.347900   \n",
            "11901  ZEBBWP4RC77B.csv  col2  KVX5NY2O0SXZ.csv  col0         0.308594   \n",
            "8841   JXIXDI3SY5A2.csv  col2  BUFVSABPBB4D.csv  col1         0.449707   \n",
            "7501   8AJ4AS53WPTR.csv  col2  SL6LA040Z20T.csv  col1         0.502441   \n",
            "19945  SQ6DL3J8ECDH.csv  col1  EYSCVHJ5A78O.csv  col2         0.374756   \n",
            "3431   JHXJ2E2E9OC5.csv  col1  P69MPMPO6F46.csv  col1         0.495361   \n",
            "21244  69VWZC71IF40.csv  col0  NBOTSA7FFMZH.csv  col2         0.450684   \n",
            "25450  KIG7OTFNAHPV.csv  col2  KLDDZNI9B1FV.csv  col0         0.405273   \n",
            "2939   N42XG1ZLIE9B.csv  col1  FGYJ1ASA9036.csv  col4         0.546387   \n",
            "23547  DQBSUQ1E8JAG.csv  col0  KIG7OTFNAHPV.csv  col1         0.313477   \n",
            "13710  FNHDFYEVJP67.csv  col1  M1GD2W3SGQZD.csv  col1         0.533203   \n",
            "4134   KUMLVVQLPUVD.csv  col0  KIG7OTFNAHPV.csv  col2         0.250000   \n",
            "9057   KIG7OTFNAHPV.csv  col1  7KRTTMFWOKRP.csv  col0         0.194214   \n",
            "\n",
            "       join_score_pert     delta  abs_delta  \n",
            "3351          0.558594  0.397339   0.397339  \n",
            "2398          0.623535  0.285645   0.285645  \n",
            "5782          0.458984  0.275879   0.275879  \n",
            "8442          0.696289  0.262695   0.262695  \n",
            "21496         0.718750  0.259277   0.259277  \n",
            "23548         0.570801  0.257568   0.257568  \n",
            "3199          0.678711  0.253662   0.253662  \n",
            "17950         0.599121  0.251221   0.251221  \n",
            "11901         0.552246  0.243652   0.243652  \n",
            "8841          0.689941  0.240234   0.240234  \n",
            "7501          0.741699  0.239258   0.239258  \n",
            "19945         0.613281  0.238525   0.238525  \n",
            "3431          0.732422  0.237061   0.237061  \n",
            "21244         0.687500  0.236816   0.236816  \n",
            "25450         0.641602  0.236328   0.236328  \n",
            "2939          0.310791 -0.235596   0.235596  \n",
            "23547         0.548828  0.235352   0.235352  \n",
            "13710         0.768555  0.235352   0.235352  \n",
            "4134          0.483887  0.233887   0.233887  \n",
            "9057          0.427734  0.233521   0.233521  \n"
          ]
        }
      ],
      "source": [
        "# 1) Load gold and perturbed scores\n",
        "gold = pd.read_csv(\"/content/drive/MyDrive/NLP/join-scores/wiki-jaccard.csv\")\n",
        "pert = pd.read_csv(\"/content/drive/MyDrive/NLP/join-scores/wiki-jaccard-perturbed.csv\")\n",
        "\n",
        "# 2) Merge on table/column identifiers\n",
        "df = (gold\n",
        "      .merge(pert, on=[\"table1\",\"col1\",\"table2\",\"col2\"],\n",
        "             suffixes=(\"_gold\",\"_pert\")))\n",
        "\n",
        "# 3) Compute difference metrics\n",
        "df[\"delta\"]       = df[\"join_score_pert\"] - df[\"join_score_gold\"]\n",
        "df[\"abs_delta\"]   = df[\"delta\"].abs()\n",
        "\n",
        "# 4) Summary statistics\n",
        "summary = {\n",
        "    \"mean_delta\":     df[\"delta\"].mean(),\n",
        "    \"mean_abs_delta\": df[\"abs_delta\"].mean(),\n",
        "    \"max_abs_delta\":  df[\"abs_delta\"].max(),\n",
        "    \"num_changed\":    (df[\"abs_delta\"] > 0).sum(),\n",
        "    \"total_pairs\":    len(df)\n",
        "}\n",
        "print(summary)\n",
        "\n",
        "# 5) (Optional) Sort by biggest changes\n",
        "top_changes = df.sort_values(\"abs_delta\", ascending=False).head(20)\n",
        "print(top_changes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
